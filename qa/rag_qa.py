import os
import requests
from config import GLM_API_KEY
from langchain.vectorstores import FAISS
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from config import VECTOR_DIR, MAX_TOP_K, SCORE_THRESHOLD

def load_index():
    if not os.path.exists(os.path.join(VECTOR_DIR, "index.faiss")):
        raise FileNotFoundError("æœªæ‰¾åˆ°å‘é‡åº“ index.faiss")
    return FAISS.load_local(VECTOR_DIR, embedding, allow_dangerous_deserialization=True)

def search_index(index, query):
    docs_and_scores = index.similarity_search_with_score(query, k=MAX_TOP_K * 2)
    filtered = [(doc, score) for doc, score in docs_and_scores if score >= SCORE_THRESHOLD]
    filtered = sorted(filtered, key=lambda x: -x[1])  # æŒ‰å¾—åˆ†é™åºæ’åˆ—
    top_k = filtered[:MAX_TOP_K]
    return top_k

def build_prompt(query, docs):
    context_text = ""
    references = []
    for i, (doc, score) in enumerate(docs):
        metadata = doc.metadata
        context_text += f"[æ–‡æ¡£{i+1}] {doc.page_content}\n"
        references.append(metadata)
    prompt = f"ä»¥ä¸‹æ˜¯è§„èŒƒæ–‡æ¡£å†…å®¹ï¼Œè¯·æ ¹æ®è¿™äº›å†…å®¹å›ç­”é—®é¢˜ã€‚\n\n{context_text}\n\né—®é¢˜ï¼š{query}\n\nè¯·åŸºäºæ–‡æ¡£å›ç­”ï¼Œä¸è¦ç¼–é€ ã€‚\n"
    return prompt, references

# ----------- é…ç½®åŒº -----------
ZHIPU_CHAT_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
MODEL_NAME = "glm-4"
TOKENIZER_MODE = "api" # è°ƒç”¨glm-apiä½¿ç”¨"api"; æœ¬åœ°éƒ¨ç½²Qwenä½¿ç”¨ "local"

# ----------- å‘é‡åº“åŠ è½½ -----------
embedding = HuggingFaceEmbeddings(model_name="BAAI/bge-large-zh")
db = FAISS.load_local(VECTOR_DIR, embedding, allow_dangerous_deserialization=True)

# ----------- æœ¬åœ°æ¨¡å‹åŠ è½½ -----------
if (TOKENIZER_MODE == "local"):
    # é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ Qwen1.5-1.8B-Chat
    LOCAL_MODEL_NAME = "Qwen/Qwen1.5-1.8B-Chat"

    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_NAME, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_NAME, trust_remote_code=True).eval()  # é»˜è®¤ CPU

    # ä½¿ç”¨ pipeline ç®€åŒ–è°ƒç”¨
    chat_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=-1  # -1 è¡¨ç¤º CPUï¼ŒM1 Air ä¸Šé»˜è®¤å³å¯
    )

# ----------- é—®ç­”å‡½æ•° -----------
def query_rag(query, chat_history):
    # 1. æ£€ç´¢æ–‡æ¡£ç›¸å…³å†…å®¹
    index = load_index()
    top_docs = search_index(index, query)
    prompt, references = build_prompt(query, top_docs)

    # 2. åŠ å…¥ context åˆ° messages
    # system + æ‰€æœ‰å†å² + å½“å‰ç”¨æˆ·é—®é¢˜
    if (TOKENIZER_MODE == "api"):
        messages = chat_history + [{"role": "user", "content": prompt}]

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {GLM_API_KEY}"
        }

        body = {
            "model": MODEL_NAME,
            "messages": messages,
            "temperature": 0.2
        }
    else:
        history_text = ""
        for message in chat_history:
            if message["role"] == "user":
                history_text += f"ç”¨æˆ·ï¼š{message['content']}\n"
            elif message["role"] == "assistant":
                history_text += f"åŠ©æ‰‹ï¼š{message['content']}\n"

        # 3. æ‹¼æ¥å®Œæ•´ promptï¼Œç¬¦åˆ Qwen å¯¹è¯æ ¼å¼
        full_prompt = history_text + f"ç”¨æˆ·ï¼š{prompt}\nåŠ©æ‰‹ï¼š"


    try:
        if (TOKENIZER_MODE == "api"):
            resp = requests.post(ZHIPU_CHAT_URL, json=body, headers=headers)
            res = resp.json()
            if "choices" in res:
                reply = res["choices"][0]["message"]["content"]
                # å°†æœ¬è½®é—®ç­”åŠ å…¥å†å²
                chat_history.append({"role": "user", "content": query})
                chat_history.append({"role": "assistant", "content": reply})
                # ref_text = "\n\nğŸ“ å‚è€ƒæ¥æºï¼š\n" + "\n".join(references)
                return reply, references
            else:
                print("è¿”å›ç»“æ„å¼‚å¸¸:", res)
                return "æ¥å£è¿”å›å¼‚å¸¸ã€‚", ""
        
        else:
            outputs = chat_pipeline(
                full_prompt,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                top_p=0.9
            )

            # å»æ‰ prompt éƒ¨åˆ†ï¼Œä¿ç•™æ¨¡å‹å›å¤
            reply = outputs[0]['generated_text'][len(full_prompt):].strip()

            # 5. æ›´æ–° chat_history
            chat_history.append({"role": "user", "content": query})
            chat_history.append({"role": "assistant", "content": reply})
        
            return reply, references 

    except Exception as e:
        print("æ¨¡å‹æ¨ç†å¤±è´¥:", e)
        return "æ— æ³•è¿æ¥å¤§æ¨¡å‹ã€‚", ""

# ---- CLI äº¤äº’ ----
if __name__ == "__main__":
    # ----------- ä¿å­˜å†å²æ¶ˆæ¯ -----------
    message_history = []
    print("æ™ºè°± RAG é—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒå¤šè½®å¯¹è¯ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰")
    while True:
        query = input("\nè¯·è¾“å…¥é—®é¢˜ï¼š")
        if query.strip().lower() in ["q", "quit", "exit"]:
            print("é€€å‡ºã€‚")
            break
        answer = query_rag(query, message_history)
        print("\nchatGLM å›ç­”ï¼š\n", answer)
