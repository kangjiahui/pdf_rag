import os
import requests
from config import GLM_API_KEY
from langchain.vectorstores import FAISS
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from config import VECTOR_DIR, MAX_TOP_K, SCORE_THRESHOLD

def load_index():
    if not os.path.exists(os.path.join(VECTOR_DIR, "index.faiss")):
        raise FileNotFoundError("æœªæ‰¾åˆ°å‘é‡åº“ index.faiss")
    return FAISS.load_local(VECTOR_DIR, embedding, allow_dangerous_deserialization=True)

def search_index(index, query):
    docs_and_scores = index.similarity_search_with_score(query, k=MAX_TOP_K * 2)
    filtered = [(doc, score) for doc, score in docs_and_scores if score >= SCORE_THRESHOLD]
    filtered = sorted(filtered, key=lambda x: -x[1])  # æŒ‰å¾—åˆ†é™åºæ’åˆ—
    top_k = filtered[:MAX_TOP_K]
    return top_k

def build_prompt(query, docs):
    context_text = ""
    references = []
    for i, (doc, score) in enumerate(docs):
        metadata = doc.metadata
        context_text += f"[æ–‡æ¡£{i+1}] {doc.page_content}\n"
        ref = f"[æ–‡æ¡£{i+1}] {metadata.get('source', '')} | {metadata.get('chapter', '')} | ç¬¬ {metadata.get('start_page', '')} - {metadata.get('end_page', '')} é¡µ"
        references.append(ref)
    prompt = f"ä»¥ä¸‹æ˜¯è§„èŒƒæ–‡æ¡£å†…å®¹ï¼Œè¯·æ ¹æ®è¿™äº›å†…å®¹å›ç­”é—®é¢˜ã€‚\n\n{context_text}\n\né—®é¢˜ï¼š{query}\n\nè¯·åŸºäºæ–‡æ¡£å›ç­”ï¼Œä¸è¦ç¼–é€ ã€‚\n"
    return prompt, references

# ----------- é…ç½®åŒº -----------
ZHIPU_CHAT_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
MODEL_NAME = "glm-4"

# ----------- å‘é‡åº“åŠ è½½ -----------
embedding = HuggingFaceEmbeddings(model_name="BAAI/bge-large-zh")
db = FAISS.load_local(VECTOR_DIR, embedding, allow_dangerous_deserialization=True)

# ----------- ä¿å­˜å†å²æ¶ˆæ¯ -----------
message_history = []

# ----------- é—®ç­”å‡½æ•° -----------
def query_rag(question: str, top_k: int = 5):
    # 1. æ£€ç´¢æ–‡æ¡£ç›¸å…³å†…å®¹
    index = load_index()
    top_docs = search_index(index, query)
    prompt, references = build_prompt(query, top_docs)

    # 2. åŠ å…¥ context åˆ° messages
    # system + æ‰€æœ‰å†å² + å½“å‰ç”¨æˆ·é—®é¢˜
    messages = message_history + [{"role": "user", "content": prompt}]

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {GLM_API_KEY}"
    }

    body = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": 0.2
    }

    try:
        resp = requests.post(ZHIPU_CHAT_URL, json=body, headers=headers)
        res = resp.json()

        if "choices" in res:
            reply = res["choices"][0]["message"]["content"]
            # å°†æœ¬è½®é—®ç­”åŠ å…¥å†å²
            message_history.append({"role": "user", "content": question})
            message_history.append({"role": "assistant", "content": reply})
            ref_text = "\n\nğŸ“ å‚è€ƒæ¥æºï¼š\n" + "\n".join(references)
            return reply + ref_text
        else:
            print("è¿”å›ç»“æ„å¼‚å¸¸:", res)
            return "âš ï¸ æ¥å£è¿”å›å¼‚å¸¸ã€‚"

    except Exception as e:
        print("è¯·æ±‚å¤±è´¥:", e)
        return "âš ï¸ æ— æ³•è¿æ¥å¤§æ¨¡å‹ã€‚"

# ---- CLI äº¤äº’ ----
if __name__ == "__main__":
    print("æ™ºè°± RAG é—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒå¤šè½®å¯¹è¯ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰")
    while True:
        query = input("\nè¯·è¾“å…¥é—®é¢˜ï¼š")
        if query.strip().lower() in ["q", "quit", "exit"]:
            print("é€€å‡ºã€‚")
            break
        answer = query_rag(query)
        print("\nchatGLM å›ç­”ï¼š\n", answer)
